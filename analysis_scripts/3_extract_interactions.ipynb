{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import ML library, initialize GPBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import gpboost\n",
    "import gpboost as gpb\n",
    "\n",
    "class GPBoostClassifier:\n",
    "    def __init__(self,scan=False,use_coords=False,random_state=42,boosting_type='gbdt'):\n",
    "        self.scan=scan\n",
    "        self.use_coords=use_coords\n",
    "        self.random_state=random_state\n",
    "        self.boosting_type=boosting_type\n",
    "    \n",
    "    def fit(self,X,y,groups,coords=None):\n",
    "        data_train = gpb.Dataset(X, y)\n",
    "        self.gp_model = gpb.GPModel(group_data=groups, likelihood=\"bernoulli_logit\", gp_coords=coords.values if self.use_coords else None,cov_function=\"exponential\")\n",
    "        params = {'learning_rate': 1e1, 'min_data_in_leaf': 20, 'objective': \"binary\",\n",
    "                  'verbose': 0}\n",
    "        if self.boosting_type!='gbdt':\n",
    "            assert self.boosting_type in ['rf','dart']\n",
    "            params['boosting']=self.boosting_type\n",
    "        params['n_jobs']=1\n",
    "        num_boost_round = 600\n",
    "        \n",
    "        if self.scan:\n",
    "            param_grid_small = {'learning_rate': [0.1,0.01,0.001], 'min_data_in_leaf': [20,50,100],\n",
    "                                'max_depth': [5,10,15], 'max_bin': [255,1000], 'use_nesterov_acc': [False,True]}\n",
    "            opt_params = gpb.grid_search_tune_parameters(param_grid=param_grid_small,\n",
    "                                                         params=params,\n",
    "                                                         num_try_random=15,\n",
    "                                                         folds=list(GroupShuffleSplit(random_state=42).split(X,y,groups)),\n",
    "                                                         gp_model=self.gp_model,\n",
    "                                                         use_gp_model_for_validation=True,\n",
    "                                                         train_set=data_train,\n",
    "                                                         verbose_eval=1,\n",
    "                                                         num_boost_round=num_boost_round,#50 \n",
    "                                                         early_stopping_rounds=10,\n",
    "                                                         seed=1,\n",
    "                                                         metrics='binary_logloss') \n",
    "\n",
    "            params=opt_params['best_params']\n",
    "\n",
    "        self.gpm = gpb.train(params=params,\n",
    "                    train_set=data_train,\n",
    "                    gp_model=self.gp_model,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    \n",
    "                   )\n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self,X,groups,coords=None):\n",
    "        y_pred = self.gpm.predict(data=X, group_data_pred=groups, gp_coords_pred=coords.values if self.use_coords else None,\n",
    "                            predict_var=True, raw_score=False)['response_mean']\n",
    "        return y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadr\n",
    "from sklearn.model_selection import GroupShuffleSplit,StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "covar=['age','sex','MLH1']\n",
    "expr=pd.read_pickle(\"../../data/dsp_data_igg_expr.pkl\")\n",
    "pheno=pd.read_pickle(\"../../data/dsp_data_igg_pheno.pkl\")\n",
    "pheno['sex']=(pheno['sex']==\"M\").astype(int)\n",
    "for k in pheno['macro_annot'].unique():\n",
    "    pheno[f'macro_{k}']=(pheno['macro_annot']==k).astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit models by each architecture, extract interactions using SHAP and save to file to read into R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tqdm\n",
    "import shap\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from functools import reduce\n",
    "from kneed import KneeLocator\n",
    "\n",
    "def fit_model(expr,pheno,macro,outcome):\n",
    "    use_macro=(macro==\"overall\")\n",
    "    if not use_macro:\n",
    "        expr=expr[pheno['macro_annot']==macro]\n",
    "        pheno=pheno[pheno['macro_annot']==macro]\n",
    "    if outcome==\"ln_only\":\n",
    "        expr=expr[pheno['Distant_Mets']==0]\n",
    "        pheno=pheno[pheno['Distant_Mets']==0]\n",
    "    elif outcome==\"Distant_Mets\":\n",
    "        expr=expr[pheno['ln_only']==0]\n",
    "        pheno=pheno[pheno['ln_only']==0]\n",
    "    covar=['age','sex','MLH1']+([] if not use_macro else ['macro_inter','macro_intra'])\n",
    "    ss=StratifiedShuffleSplit(n_splits=10, random_state=42, test_size=None,\n",
    "                    train_size=0.7)\n",
    "    data_dict=[]\n",
    "    for i,(train_idx,test_idx) in enumerate(list(ss.split(X=expr,y=pheno[outcome],groups=pheno['batch']))):\n",
    "        data_dict_=dict()\n",
    "        data_dict_['expr_train'],data_dict_['pheno_train']=expr.iloc[train_idx],pheno.iloc[train_idx]\n",
    "        data_dict_['expr_test'],data_dict_['pheno_test']=expr.iloc[test_idx],pheno.iloc[test_idx]\n",
    "        data_dict.append(data_dict_)\n",
    "\n",
    "    results=[]\n",
    "    mods=[]\n",
    "    for cv_idx in tqdm.trange(len(data_dict)):\n",
    "        np.random.seed(42)\n",
    "        data_dict_=data_dict[i].copy()\n",
    "        X_train,X_test,y_train,y_test,g_train,g_test=pd.concat([data_dict_['expr_train'],data_dict_['pheno_train'][covar]],axis=1),\\\n",
    "                                                        pd.concat([data_dict_['expr_test'],data_dict_['pheno_test'][covar]],axis=1),\\\n",
    "                                                        data_dict_['pheno_train'][outcome],\\\n",
    "                                                        data_dict_['pheno_test'][outcome],\\\n",
    "                                                        data_dict_['pheno_train']['batch'],\\\n",
    "                                                        data_dict_['pheno_test']['batch']\n",
    "        gpc=GPBoostClassifier(random_state=42).fit(X_train,y_train,g_train)\n",
    "        y_pred=gpc.predict_proba(X_test,y_test,g_test)\n",
    "        results.append(dict(y_pred=y_pred,\n",
    "                              y_true=y_test.astype(float),\n",
    "                               data_dict=data_dict_))#,mod=gpc\n",
    "        mods.append(gpc)\n",
    "\n",
    "    pickle.dump(results,open(f\"./analyses/4_MEML/pickle_res/gpbres-{macro}-{outcome}.pkl\",'wb'))\n",
    "    print(np.mean([roc_auc_score(d_['y_true'],d_['y_pred']) for d_ in results]))\n",
    "    \n",
    "    explainers=[]\n",
    "    shap_vals=[]\n",
    "    shap_interactions=[]\n",
    "\n",
    "    for cv_idx in tqdm.trange(len(data_dict)):\n",
    "        gpc=mods[cv_idx]\n",
    "        explainers.append(shap.TreeExplainer(gpc.gpm))\n",
    "        data_dict_=results[cv_idx]['data_dict'].copy()\n",
    "        X_train=pd.concat([data_dict_['expr_train'],data_dict_['pheno_train'][covar]],axis=1)\n",
    "        shap_vals.append(explainers[-1].shap_values(X_train,data_dict_['pheno_train']['batch'],check_additivity=False))\n",
    "        shap_interactions.append(explainers[-1].shap_interaction_values(X_train,data_dict_['pheno_train']['batch']))\n",
    "    pickle.dump(dict(shap_vals=shap_vals,\n",
    "                shap_interactions=shap_interactions),open(f\"./analyses/4_MEML/pickle_res/shap_interactions-{macro}-{outcome}.pkl\",'wb'))\n",
    "\n",
    "    index_col=expr.columns.tolist()+covar\n",
    "    df_interaction=pd.DataFrame(np.abs(np.concatenate(shap_interactions,0)).mean(0),index=index_col,columns=index_col)\n",
    "    df_interaction[np.eye(*df_interaction.shape).astype(bool)]=0\n",
    "    all_interaction_shap_scores=reduce(lambda x,y:x+y,[df_interaction]).where(np.triu(np.ones(df_interaction.shape),k=1).astype(np.bool)).stack().reset_index()\n",
    "    all_interaction_shap_scores.columns=['feature_1','feature_2', 'shap_interaction_score']\n",
    "    all_interaction_shap_scores=all_interaction_shap_scores.sort_values('shap_interaction_score',ascending=False)\n",
    "    kneed=KneeLocator(np.arange(all_interaction_shap_scores.shape[0]), all_interaction_shap_scores['shap_interaction_score'], direction='decreasing', curve='convex',\n",
    "                      S=150.0)\n",
    "    n_top_interactions=min(600,kneed.knee)\n",
    "    form_str=['+'.join(index_col),'+'.join(map(lambda x:f\"{x[0]}:{x[1]}\",all_interaction_shap_scores.iloc[:n_top_interactions].iloc[:,:2].values.tolist()))]\n",
    "    pd.to_pickle(form_str,f\"./analyses/4_MEML/pickle_res/form_str-{macro}-{outcome}.pkl\")\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 10min 13.0s\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "with ProgressBar():\n",
    "    finished=dask.compute(*[ dask.delayed(fit_model)(expr,pheno,macro,outcome) for outcome in ['any_mets','ln_only','Distant_Mets'] for macro in ['overall','inter','intra','away']],scheduler=\"processes\")\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
